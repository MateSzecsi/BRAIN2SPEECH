{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prepare and FC_DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UAvZxk4e9KOR"
   },
   "outputs": [],
   "source": [
    "# Import data_prepare modul that contains functions that we created.\n",
    "from data_prepare import *\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Flatten, Conv1D, MaxPooling1D, Dropout\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from scripts.reconstruction_minimal import createAudio\n",
    "import scipy.io.wavfile as wavfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0ZuYd79QvvG"
   },
   "source": [
    "# Prepare data for learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare the data for learning neural networks. We use a function that execute the results of the last milestone get a train-ready dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_datasets(pt, feat_path=r'./features', cnn_prepare=False):\n",
    "    \n",
    "    r\"\"\"\n",
    "    Generate train and test datasets from the one sample of the raw data.\n",
    "    param pt: name of the sample\n",
    "    param feat_path: path of the raw data file\n",
    "  \"\"\"\n",
    "    \n",
    "    spectrogram = np.load(os.path.join(feat_path,f'{pt}_spec.npy'))\n",
    "    data = np.load(os.path.join(feat_path,f'{pt}_feat.npy'))\n",
    "    \n",
    "    # Reduce Dimensions\n",
    "    pca = PCA()\n",
    "    pca.fit(data)\n",
    "    data = np.dot(data, pca.components_[:50,:].T)\n",
    "    \n",
    "    if cnn_prepare:\n",
    "        # Creating data for CNN: stacking window_size input data and connecting it to the output\n",
    "        # at the end of the window\n",
    "        window_size = 4\n",
    "        input = np.zeros((data.shape[0] - window_size, window_size, data.shape[1]))\n",
    "        output = np.zeros((spectrogram.shape[0] - window_size, spectrogram.shape[1]))\n",
    "        for i in range(input.shape[0] - window_size):\n",
    "          output[i, :] = spectrogram[i + window_size, :]\n",
    "          input[i, :, :] = data[i:i+window_size, :]\n",
    "    else:\n",
    "        output = spectrogram\n",
    "        input = data\n",
    "\n",
    "    # Generate train, validation and test datasets\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(input, output, test_size=0.1, shuffle=False)\n",
    "\n",
    "    # Shuffle train data\n",
    "    randomize = np.arange(len(X_train))\n",
    "    np.random.shuffle(randomize)\n",
    "    X_train = X_train[randomize]\n",
    "    Y_train = Y_train[randomize]\n",
    "    \n",
    "    # Standardize input\n",
    "    mu_input = np.mean(X_train, axis=0)\n",
    "    std_input = np.std(X_train, axis=0)\n",
    "    X_train = (X_train-mu_input)/std_input\n",
    "    X_test = (X_test-mu_input)/std_input\n",
    "\n",
    "    # Standardize output\n",
    "    mu_output = np.mean(Y_train, axis=0)\n",
    "    std_output = np.std(Y_train, axis=0)\n",
    "    Y_train = (Y_train-mu_output)/std_output\n",
    "    Y_test = (Y_test-mu_output)/std_output\n",
    "\n",
    "\n",
    "    return (X_train, Y_train, X_test, Y_test, mu_input, std_input, mu_output, std_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_merged_dataset(cnn_prepare=False):\n",
    "    norm_params = {}\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "    pts = ['sub-%02d'%i for i in range(1,11)]\n",
    "    (X_train, Y_train, x_test, y_test, mu_input, std_input, mu_output, std_output) = generate_datasets(pts[0], cnn_prepare=cnn_prepare)\n",
    "    norm_params[str(0)]={'mu_input':mu_input, 'std_input':std_input, 'mu_output':mu_output, 'std_output':std_output}\n",
    "    X_test.append(x_test)\n",
    "    Y_test.append(y_test)\n",
    "    for i, pt in enumerate(pts[1:]):\n",
    "        (x_train, y_train, x_test, y_test, mu_input, std_input, mu_output, std_output) = generate_datasets(pt, cnn_prepare=cnn_prepare)\n",
    "        norm_params[str(i+1)]={'mu_input':mu_input, 'std_input':std_input, 'mu_output':mu_output, 'std_output':std_output}\n",
    "        X_train = np.concatenate((X_train, x_train))\n",
    "        Y_train = np.concatenate((Y_train, y_train))\n",
    "        X_test.append(x_test)\n",
    "        Y_test.append(y_test)\n",
    "    return (X_train, Y_train, X_test, Y_test, norm_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create FC-DNN network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fndnn(input_dim, output_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=150, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units=100, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units=80, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units=60, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units=30, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units=10, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units=output_dim, activation='linear'))\n",
    "\n",
    "    optimizer=Adam(learning_rate=0.001)\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_1d_convnet(window_size, filter_length, nb_input_series=1, nb_outputs=1, nb_filter=[40, 40]):\n",
    "    #probably more filter layers should be added\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=nb_filter[0], kernel_size=filter_length, activation='relu', input_shape=(window_size, nb_input_series)))\n",
    "    model.add(Conv1D(filters=nb_filter[1], kernel_size=filter_length, activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(nb_outputs, activation='linear'))\n",
    "\n",
    "    optimizer=Adam(learning_rate=0.001)\n",
    "    \n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, X_test, Y_test, norm_params):\n",
    "    preds = []\n",
    "    spectograms = []\n",
    "    losses = []\n",
    "    for i in range(10):\n",
    "        y_pred = model.predict(X_test[i])\n",
    "        losses.append(mean_absolute_error(Y_test[i], y_pred))\n",
    "        preds.append(y_pred * norm_params[str(i)]['std_output'] + norm_params[str(i)]['mu_output'])\n",
    "        spectograms.append(Y_test[i] * norm_params[str(i)]['std_output'] + norm_params[str(i)]['mu_output'])\n",
    "    mae = np.sum(losses)\n",
    "    print(f'Mean absolute error of test set:{mae}')\n",
    "    return preds, spectograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.90327, saving model to cnn.hdf5\n",
      "472/472 - 5s - loss: 0.9280 - mae: 0.7285 - val_loss: 0.9033 - val_mae: 0.6071 - 5s/epoch - 10ms/step\n",
      "Epoch 2/1000\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.90327\n",
      "472/472 - 3s - loss: 0.8281 - mae: 0.6709 - val_loss: 0.9385 - val_mae: 0.6163 - 3s/epoch - 7ms/step\n",
      "Epoch 3/1000\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.90327\n",
      "472/472 - 3s - loss: 0.7840 - mae: 0.6440 - val_loss: 0.9809 - val_mae: 0.6334 - 3s/epoch - 7ms/step\n",
      "Epoch 4/1000\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.90327\n",
      "472/472 - 3s - loss: 0.7586 - mae: 0.6292 - val_loss: 0.9670 - val_mae: 0.6168 - 3s/epoch - 7ms/step\n",
      "Epoch 5/1000\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.90327\n",
      "472/472 - 3s - loss: 0.7405 - mae: 0.6189 - val_loss: 0.9523 - val_mae: 0.6158 - 3s/epoch - 7ms/step\n",
      "Epoch 6/1000\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.90327\n",
      "472/472 - 3s - loss: 0.7256 - mae: 0.6099 - val_loss: 0.9589 - val_mae: 0.6184 - 3s/epoch - 7ms/step\n",
      "Epoch 7/1000\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.90327\n",
      "472/472 - 3s - loss: 0.7135 - mae: 0.6036 - val_loss: 0.9884 - val_mae: 0.6212 - 3s/epoch - 7ms/step\n",
      "Epoch 8/1000\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.90327\n",
      "472/472 - 3s - loss: 0.7037 - mae: 0.5982 - val_loss: 0.9902 - val_mae: 0.6279 - 3s/epoch - 6ms/step\n",
      "Epoch 9/1000\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.90327\n",
      "472/472 - 3s - loss: 0.6947 - mae: 0.5932 - val_loss: 0.9889 - val_mae: 0.6237 - 3s/epoch - 7ms/step\n",
      "Epoch 10/1000\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.90327\n",
      "472/472 - 3s - loss: 0.6864 - mae: 0.5888 - val_loss: 0.9743 - val_mae: 0.6256 - 3s/epoch - 6ms/step\n",
      "Epoch 11/1000\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.90327\n",
      "472/472 - 3s - loss: 0.6783 - mae: 0.5850 - val_loss: 0.9826 - val_mae: 0.6331 - 3s/epoch - 7ms/step\n",
      "Epoch 12/1000\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.90327\n",
      "472/472 - 3s - loss: 0.6712 - mae: 0.5811 - val_loss: 0.9814 - val_mae: 0.6340 - 3s/epoch - 7ms/step\n",
      "Epoch 13/1000\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.90327\n",
      "472/472 - 3s - loss: 0.6653 - mae: 0.5777 - val_loss: 0.9806 - val_mae: 0.6287 - 3s/epoch - 7ms/step\n",
      "Epoch 14/1000\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.90327\n",
      "472/472 - 3s - loss: 0.6595 - mae: 0.5749 - val_loss: 0.9918 - val_mae: 0.6297 - 3s/epoch - 7ms/step\n",
      "Epoch 15/1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m checkpointer\u001b[38;5;241m=\u001b[39mModelCheckpoint(filepath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcnn.hdf5\u001b[39m\u001b[38;5;124m'\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     13\u001b[0m model_cnn \u001b[38;5;241m=\u001b[39m make_1d_convnet(window_size\u001b[38;5;241m=\u001b[39mwindow_size, filter_length\u001b[38;5;241m=\u001b[39mfilter_length, nb_filter\u001b[38;5;241m=\u001b[39mnb_filter, nb_input_series\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, nb_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m23\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_cnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpointer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m preds_cnn, spectograms \u001b[38;5;241m=\u001b[39m test_model(model_cnn, X_test, Y_test, norm_params)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py:1413\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1411\u001b[0m   context\u001b[38;5;241m.\u001b[39masync_wait()\n\u001b[0;32m   1412\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs  \u001b[38;5;66;03m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[1;32m-> 1413\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m \u001b[43mdata_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_increment\u001b[49m\n\u001b[0;32m   1414\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(end_step, logs)\n\u001b[0;32m   1415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\data_adapter.py:1268\u001b[0m, in \u001b[0;36mDataHandler.step_increment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1265\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m steps_remaining\n\u001b[0;32m   1266\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution\u001b[38;5;241m.\u001b[39massign(original_spe)\n\u001b[1;32m-> 1268\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m   1269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_increment\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1270\u001b[0m   \u001b[38;5;124;03m\"\"\"The number to increment the step for `on_batch_end` methods.\"\"\"\u001b[39;00m\n\u001b[0;32m   1271\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_increment\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "(X_train, Y_train, X_test, Y_test, norm_params) = generate_merged_dataset(cnn_prepare=True)\n",
    "\n",
    "#Parameters for the CNN network\n",
    "# one data point covers 2 * 200 ms of eeg signal\n",
    "filter_length = 2 #covers 4*8*400 sec of eeg\n",
    "window_size = 2 * 2 #we have two 1d max pool layers\n",
    "nb_filter = [40, 40]\n",
    "epochs = 1000\n",
    "batch_size = 512\n",
    "\n",
    "early_stopping=EarlyStopping(patience=50, verbose=1)\n",
    "checkpointer=ModelCheckpoint(filepath='cnn.hdf5', save_best_only=True, verbose=1)\n",
    "model_cnn = make_1d_convnet(window_size=window_size, filter_length=filter_length, nb_filter=nb_filter, nb_input_series=50, nb_outputs=23)\n",
    "history = model_cnn.fit(X_train, Y_train, validation_split=0.1, epochs=epochs, batch_size=batch_size, verbose=2, callbacks=[checkpointer, early_stopping])\n",
    "\n",
    "preds_cnn, spectograms = test_model(model_cnn, X_test, Y_test, norm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.99922, saving model to fcdnn.hdf5\n",
      "472/472 - 8s - loss: 1.0102 - mae: 0.7779 - val_loss: 0.9992 - val_mae: 0.6534 - 8s/epoch - 17ms/step\n",
      "Epoch 2/1000\n",
      "\n",
      "Epoch 2: val_loss improved from 0.99922 to 0.99822, saving model to fcdnn.hdf5\n",
      "472/472 - 7s - loss: 0.9993 - mae: 0.7748 - val_loss: 0.9982 - val_mae: 0.6473 - 7s/epoch - 14ms/step\n",
      "Epoch 3/1000\n",
      "\n",
      "Epoch 3: val_loss improved from 0.99822 to 0.98615, saving model to fcdnn.hdf5\n",
      "472/472 - 6s - loss: 0.9880 - mae: 0.7685 - val_loss: 0.9862 - val_mae: 0.6251 - 6s/epoch - 13ms/step\n",
      "Epoch 4/1000\n",
      "\n",
      "Epoch 4: val_loss improved from 0.98615 to 0.97740, saving model to fcdnn.hdf5\n",
      "472/472 - 7s - loss: 0.9747 - mae: 0.7585 - val_loss: 0.9774 - val_mae: 0.6152 - 7s/epoch - 14ms/step\n",
      "Epoch 5/1000\n",
      "\n",
      "Epoch 5: val_loss improved from 0.97740 to 0.97543, saving model to fcdnn.hdf5\n",
      "472/472 - 7s - loss: 0.9639 - mae: 0.7518 - val_loss: 0.9754 - val_mae: 0.6073 - 7s/epoch - 14ms/step\n",
      "Epoch 6/1000\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.97543\n",
      "472/472 - 6s - loss: 0.9495 - mae: 0.7425 - val_loss: 0.9833 - val_mae: 0.6028 - 6s/epoch - 14ms/step\n",
      "Epoch 7/1000\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.97543\n",
      "472/472 - 7s - loss: 0.9393 - mae: 0.7366 - val_loss: 0.9817 - val_mae: 0.5996 - 7s/epoch - 14ms/step\n",
      "Epoch 8/1000\n",
      "\n",
      "Epoch 8: val_loss improved from 0.97543 to 0.97343, saving model to fcdnn.hdf5\n",
      "472/472 - 7s - loss: 0.9278 - mae: 0.7293 - val_loss: 0.9734 - val_mae: 0.5956 - 7s/epoch - 14ms/step\n",
      "Epoch 9/1000\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.97343\n",
      "472/472 - 7s - loss: 0.9200 - mae: 0.7252 - val_loss: 0.9888 - val_mae: 0.5916 - 7s/epoch - 14ms/step\n",
      "Epoch 10/1000\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.97343\n",
      "472/472 - 7s - loss: 0.9110 - mae: 0.7193 - val_loss: 0.9857 - val_mae: 0.5925 - 7s/epoch - 14ms/step\n",
      "Epoch 11/1000\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.97343\n",
      "472/472 - 7s - loss: 0.9052 - mae: 0.7160 - val_loss: 0.9825 - val_mae: 0.5911 - 7s/epoch - 14ms/step\n",
      "Epoch 12/1000\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.97343\n",
      "472/472 - 7s - loss: 0.8987 - mae: 0.7127 - val_loss: 0.9806 - val_mae: 0.5877 - 7s/epoch - 14ms/step\n",
      "Epoch 13/1000\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.97343\n",
      "472/472 - 7s - loss: 0.8927 - mae: 0.7082 - val_loss: 0.9735 - val_mae: 0.5873 - 7s/epoch - 14ms/step\n",
      "Epoch 14/1000\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.97343\n",
      "472/472 - 7s - loss: 0.8874 - mae: 0.7055 - val_loss: 0.9808 - val_mae: 0.5879 - 7s/epoch - 14ms/step\n",
      "Epoch 15/1000\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.97343\n",
      "472/472 - 7s - loss: 0.8809 - mae: 0.7020 - val_loss: 0.9779 - val_mae: 0.5854 - 7s/epoch - 14ms/step\n",
      "Epoch 16/1000\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.97343\n",
      "472/472 - 7s - loss: 0.8763 - mae: 0.6987 - val_loss: 0.9758 - val_mae: 0.5871 - 7s/epoch - 14ms/step\n",
      "Epoch 17/1000\n",
      "\n",
      "Epoch 17: val_loss improved from 0.97343 to 0.97000, saving model to fcdnn.hdf5\n",
      "472/472 - 7s - loss: 0.8740 - mae: 0.6974 - val_loss: 0.9700 - val_mae: 0.5868 - 7s/epoch - 14ms/step\n",
      "Epoch 18/1000\n",
      "\n",
      "Epoch 18: val_loss improved from 0.97000 to 0.96382, saving model to fcdnn.hdf5\n",
      "472/472 - 7s - loss: 0.8701 - mae: 0.6946 - val_loss: 0.9638 - val_mae: 0.5863 - 7s/epoch - 14ms/step\n",
      "Epoch 19/1000\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.96382\n",
      "472/472 - 7s - loss: 0.8658 - mae: 0.6927 - val_loss: 0.9794 - val_mae: 0.5826 - 7s/epoch - 14ms/step\n",
      "Epoch 20/1000\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.96382\n",
      "472/472 - 7s - loss: 0.8591 - mae: 0.6890 - val_loss: 0.9741 - val_mae: 0.5811 - 7s/epoch - 14ms/step\n",
      "Epoch 21/1000\n",
      "\n",
      "Epoch 21: val_loss improved from 0.96382 to 0.96277, saving model to fcdnn.hdf5\n",
      "472/472 - 7s - loss: 0.8567 - mae: 0.6871 - val_loss: 0.9628 - val_mae: 0.5838 - 7s/epoch - 14ms/step\n",
      "Epoch 22/1000\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.96277\n",
      "472/472 - 7s - loss: 0.8543 - mae: 0.6859 - val_loss: 0.9766 - val_mae: 0.5799 - 7s/epoch - 14ms/step\n",
      "Epoch 23/1000\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.96277\n",
      "472/472 - 7s - loss: 0.8515 - mae: 0.6843 - val_loss: 0.9679 - val_mae: 0.5825 - 7s/epoch - 14ms/step\n",
      "Epoch 24/1000\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.96277\n",
      "472/472 - 7s - loss: 0.8478 - mae: 0.6819 - val_loss: 0.9677 - val_mae: 0.5845 - 7s/epoch - 14ms/step\n",
      "Epoch 25/1000\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.96277\n",
      "472/472 - 7s - loss: 0.8465 - mae: 0.6813 - val_loss: 0.9676 - val_mae: 0.5820 - 7s/epoch - 14ms/step\n",
      "Epoch 26/1000\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.96277\n",
      "472/472 - 7s - loss: 0.8453 - mae: 0.6805 - val_loss: 0.9668 - val_mae: 0.5829 - 7s/epoch - 14ms/step\n",
      "Epoch 27/1000\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.96277\n",
      "472/472 - 7s - loss: 0.8440 - mae: 0.6794 - val_loss: 0.9705 - val_mae: 0.5824 - 7s/epoch - 14ms/step\n",
      "Epoch 28/1000\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.96277\n",
      "472/472 - 6s - loss: 0.8436 - mae: 0.6799 - val_loss: 0.9710 - val_mae: 0.5787 - 6s/epoch - 14ms/step\n",
      "Epoch 29/1000\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.96277\n",
      "472/472 - 7s - loss: 0.8405 - mae: 0.6779 - val_loss: 0.9689 - val_mae: 0.5820 - 7s/epoch - 14ms/step\n",
      "Epoch 30/1000\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.96277\n",
      "472/472 - 7s - loss: 0.8377 - mae: 0.6759 - val_loss: 0.9663 - val_mae: 0.5805 - 7s/epoch - 14ms/step\n",
      "Epoch 31/1000\n"
     ]
    }
   ],
   "source": [
    "(X_train, Y_train, X_test, Y_test, norm_params) = generate_merged_dataset()\n",
    "\n",
    "#Parameters for the FC-DNN network\n",
    "epochs = 1000\n",
    "batch_size = 512\n",
    "\n",
    "early_stopping=EarlyStopping(patience=50, verbose=1)\n",
    "checkpointer=ModelCheckpoint(filepath='fcdnn.hdf5', save_best_only=True, verbose=1)\n",
    "model_fcdnn = create_fndnn(input_dim=X_train.shape[1], output_dim=Y_train.shape[1])\n",
    "history = model_fcdnn.fit(X_train, Y_train, validation_split=0.1, epochs=epochs, batch_size=batch_size, verbose=2, callbacks=[checkpointer, early_stopping])\n",
    "\n",
    "preds_fcdnn, spectograms = test_model(model_fcdnn, X_test, Y_test, norm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viz spectrograms\n",
    "def visualize_results(preds, spectograms):\n",
    "\n",
    "    cm='viridis'\n",
    "    fig, ax = plt.subplots(2, sharex=True)\n",
    "\n",
    "    for i in range(10):\n",
    "        #Plot spectrograms\n",
    "        ax[0].imshow(np.flipud(spectograms[i].T), cmap=cm, interpolation=None,aspect='auto')\n",
    "        ax[0].set_ylabel('Log Mel-Spec Bin')\n",
    "        ax[1].imshow(np.flipud(preds[i].T), cmap=cm, interpolation=None,aspect='auto')\n",
    "        ax[1].set_ylabel('Log Mel-Spec Bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthesize audio files\n",
    "def synthesize_audio(preds, spectograms, result_path):\n",
    "    winLength = 0.05\n",
    "    frameshift = 0.01\n",
    "    audiosr = 16000\n",
    "    if not os.path.isdir(result_path):\n",
    "        os.makedirs(result_path)\n",
    "    for i in range(10):\n",
    "        #Synthesize waveform from spectrogram using Griffin-Lim\n",
    "        reconstructedWav = createAudio(preds[i],audiosr=audiosr,winLength=winLength,frameshift=frameshift)\n",
    "        wavfile.write(os.path.join(result_path,f'{i+1}_predicted.wav'),int(audiosr),reconstructedWav)\n",
    "\n",
    "        #For comparison synthesize the original spectrogram with Griffin-Lim\n",
    "        origWav = createAudio(spectograms[i],audiosr=audiosr,winLength=winLength,frameshift=frameshift)\n",
    "        wavfile.write(os.path.join(result_path,f'{i+1}_orig_synthesized.wav'),int(audiosr),origWav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results(preds=preds_fcdnn, spectograms=spectograms)\n",
    "visualize_results(preds=preds_cnn, spectograms=spectograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesize_audio(preds=preds_fcdnn, spectograms=spectograms, result_path = 'dnn_results')\n",
    "synthesize_audio(preds=preds_cnn, spectograms=spectograms, result_path = 'cnn_results')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "cc66402ee9dd0d45cb681ef7bf9287b06b34cc512cacd28107dca7ce0d1247ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
