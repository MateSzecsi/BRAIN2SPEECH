{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home assignment Milestone 1 -- BRAIN2SPEECH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the project we would like to syntetise speech from brain signal.\n",
    "\n",
    "For this we use the Intractranial EEG datasets that can be found here: https://osf.io/nrgx6/\n",
    "\n",
    "We created a data_prepare.py file, that contains 1-3 helper functions for data prepare.\n",
    "\n",
    "The main tasks was:\n",
    "- Download the raw data and extract from the NWB format and create NumPy arrays from it.\n",
    "- Filter the silence periods from the signal to get more balanced dataset for deep learning.\n",
    "- Generate train, validation and test datasets from the data.\n",
    "\n",
    "We used the research of NeuralinterfacingLab as the starting point of our project, the research lab used LineaRegression for syntetise speech from the brain signal and we will use deep learning for the same exercise. During the projects we used some scripts created by NeuralinterfacingLab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UAvZxk4e9KOR"
   },
   "outputs": [],
   "source": [
    "# Import data_prepare modul that contains functions that we created.\n",
    "from data_prepare import *\n",
    "import zipfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1VnKZ7hD-LD"
   },
   "source": [
    "# Download data and scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We downloaded the raw data and extract it from the scripts that was created by NeuralinterfacingLab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "crNoXo1B9p9_"
   },
   "outputs": [],
   "source": [
    "# Download the data and extract it\n",
    "download_file(\"https://files.de-1.osf.io/v1/resources/nrgx6/providers/osfstorage/623d9d9a938b480e3797af8f\", \"data.zip\")\n",
    "zipfile.ZipFile(\"data.zip\", 'r').extractall(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4HTJdAqMGfjH",
    "outputId": "4092efa4-1d91-4a45-9bf8-7df7c769f07b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scripts/extract_features.py:131: DeprecationWarning: scipy.hanning is deprecated and will be removed in SciPy 2.0.0, use numpy.hanning instead\n",
      "  win = scipy.hanning(np.floor(windowLength*sr + 1))[:-1]\n",
      "scripts/extract_features.py:131: DeprecationWarning: scipy.hanning is deprecated and will be removed in SciPy 2.0.0, use numpy.hanning instead\n",
      "  win = scipy.hanning(np.floor(windowLength*sr + 1))[:-1]\n",
      "scripts/extract_features.py:131: DeprecationWarning: scipy.hanning is deprecated and will be removed in SciPy 2.0.0, use numpy.hanning instead\n",
      "  win = scipy.hanning(np.floor(windowLength*sr + 1))[:-1]\n",
      "scripts/extract_features.py:131: DeprecationWarning: scipy.hanning is deprecated and will be removed in SciPy 2.0.0, use numpy.hanning instead\n",
      "  win = scipy.hanning(np.floor(windowLength*sr + 1))[:-1]\n",
      "scripts/extract_features.py:131: DeprecationWarning: scipy.hanning is deprecated and will be removed in SciPy 2.0.0, use numpy.hanning instead\n",
      "  win = scipy.hanning(np.floor(windowLength*sr + 1))[:-1]\n",
      "scripts/extract_features.py:131: DeprecationWarning: scipy.hanning is deprecated and will be removed in SciPy 2.0.0, use numpy.hanning instead\n",
      "  win = scipy.hanning(np.floor(windowLength*sr + 1))[:-1]\n",
      "scripts/extract_features.py:131: DeprecationWarning: scipy.hanning is deprecated and will be removed in SciPy 2.0.0, use numpy.hanning instead\n",
      "  win = scipy.hanning(np.floor(windowLength*sr + 1))[:-1]\n",
      "scripts/extract_features.py:131: DeprecationWarning: scipy.hanning is deprecated and will be removed in SciPy 2.0.0, use numpy.hanning instead\n",
      "  win = scipy.hanning(np.floor(windowLength*sr + 1))[:-1]\n",
      "scripts/extract_features.py:131: DeprecationWarning: scipy.hanning is deprecated and will be removed in SciPy 2.0.0, use numpy.hanning instead\n",
      "  win = scipy.hanning(np.floor(windowLength*sr + 1))[:-1]\n",
      "scripts/extract_features.py:131: DeprecationWarning: scipy.hanning is deprecated and will be removed in SciPy 2.0.0, use numpy.hanning instead\n",
      "  win = scipy.hanning(np.floor(windowLength*sr + 1))[:-1]\n"
     ]
    }
   ],
   "source": [
    "# Run script for extract the downloaded data\n",
    "execfile(\"scripts/extract_features.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove unnecessary files\n",
    "os.remove(\"data.zip\")\n",
    "shutil.rmtree(\"SingleWordProductionDutch-iBIDS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0ZuYd79QvvG"
   },
   "source": [
    "# Prepare data for learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the data preparation at first we filter the silenced parts of the datasets to get a more balanced datasets that better for deep learning. After that we splited the dataset to 3 part(Train, Validation and Test). After that we standardize the datasets and we created a dimensional reduction using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FX-J_URiDokG"
   },
   "outputs": [],
   "source": [
    "# Get lists of samples\n",
    "pts = ['sub-%02d'%i for i in range(1,11)]\n",
    "# New list for the prepared datasets\n",
    "prepared_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GM7X7z8CEQku"
   },
   "outputs": [],
   "source": [
    "# Prepare data from all sample for deep learning\n",
    "for pt in pts:\n",
    "    #Prepare data for learning\n",
    "    (X_train, Y_train, X_val, Y_val, X_test, Y_test) = generate_datasets(pts[0])\n",
    "    prepared_data.append((X_train, Y_train, X_val, Y_val, X_test, Y_test))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
